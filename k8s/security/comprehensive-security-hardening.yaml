# CC03 v38.0 - Comprehensive Security Hardening
# Advanced security policies, controls, and monitoring

apiVersion: v1
kind: Namespace
metadata:
  name: security-system
  labels:
    name: security-system
    security.hardened: "true"
    version: v38.0

---
# Pod Security Standards
apiVersion: v1
kind: Namespace
metadata:
  name: itdo-erp-prod
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted

---
# Network Policy - Default Deny All
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: itdo-erp-prod
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

---
# Network Policy - Backend
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-network-policy
  namespace: itdo-erp-prod
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    - namespaceSelector:
        matchLabels:
          name: monitoring
    - namespaceSelector:
        matchLabels:
          name: security-system
    ports:
    - protocol: TCP
      port: 8000
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: postgresql
    ports:
    - protocol: TCP
      port: 5432
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379
  - to: []
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
    - protocol: TCP
      port: 443

---
# Network Policy - Frontend
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-network-policy
  namespace: itdo-erp-prod
spec:
  podSelector:
    matchLabels:
      app: frontend
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - protocol: TCP
      port: 80
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: backend
    ports:
    - protocol: TCP
      port: 8000
  - to: []
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53

---
# Network Policy - Database
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: database-network-policy
  namespace: itdo-erp-prod
spec:
  podSelector:
    matchLabels:
      app: postgresql
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: backend
    - namespaceSelector:
        matchLabels:
          name: monitoring
    - namespaceSelector:
        matchLabels:
          name: security-system
    ports:
    - protocol: TCP
      port: 5432
    - protocol: TCP
      port: 9187
  egress:
  - to: []
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53

---
# Security Policy - Pod Security Policy Replacement (Security Context Constraints)
apiVersion: v1
kind: SecurityContextConstraints
metadata:
  name: itdo-erp-restricted
  annotations:
    kubernetes.io/description: "Restricted security context constraints for ITDO ERP"
allowHostDirVolumePlugin: false
allowHostIPC: false
allowHostNetwork: false
allowHostPID: false
allowHostPorts: false
allowPrivileged: false
allowPrivilegeEscalation: false
allowedCapabilities: null
defaultAddCapabilities: null
requiredDropCapabilities:
- KILL
- MKNOD
- SETUID
- SETGID
fsGroup:
  type: MustRunAs
  ranges:
  - min: 1
    max: 65535
readOnlyRootFilesystem: true
runAsUser:
  type: MustRunAsNonRoot
seLinuxContext:
  type: MustRunAs
supplementalGroups:
  type: MustRunAs
  ranges:
  - min: 1
    max: 65535
volumes:
- configMap
- downwardAPI
- emptyDir
- persistentVolumeClaim
- projected
- secret

---
# RBAC - Security Admin Role
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: security-admin
rules:
- apiGroups: [""]
  resources: ["pods", "pods/log", "pods/exec"]
  verbs: ["get", "list", "create", "delete"]
- apiGroups: [""]
  resources: ["secrets", "configmaps"]
  verbs: ["get", "list", "create", "update", "delete"]
- apiGroups: ["networking.k8s.io"]
  resources: ["networkpolicies"]
  verbs: ["get", "list", "create", "update", "delete"]
- apiGroups: ["policy"]
  resources: ["podsecuritypolicies"]
  verbs: ["get", "list", "create", "update", "delete"]
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["roles", "rolebindings", "clusterroles", "clusterrolebindings"]
  verbs: ["get", "list", "create", "update", "delete"]

---
# Security Scanner Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: security-scanner
  namespace: security-system
  labels:
    app: security-scanner
    component: scanner
spec:
  replicas: 1
  selector:
    matchLabels:
      app: security-scanner
  template:
    metadata:
      labels:
        app: security-scanner
        component: scanner
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8090"
    spec:
      serviceAccountName: security-scanner
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
      containers:
      - name: trivy-scanner
        image: aquasec/trivy:0.44.0
        command:
        - sh
        - -c
        - |
          while true; do
            echo "Starting security scan..."
            trivy image --severity HIGH,CRITICAL --format json --output /tmp/scan-results.json itdo-erp/backend:latest
            trivy image --severity HIGH,CRITICAL --format json --output /tmp/frontend-scan-results.json itdo-erp/frontend:latest
            echo "Scan completed. Sleeping for 1 hour..."
            sleep 3600
          done
        volumeMounts:
        - name: scan-results
          mountPath: /tmp
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          capabilities:
            drop:
            - ALL
      - name: compliance-checker
        image: aquasec/kube-bench:v0.6.10
        command:
        - sh
        - -c
        - |
          while true; do
            echo "Running CIS Kubernetes Benchmark..."
            kube-bench --json > /tmp/compliance-results.json
            echo "Compliance check completed. Sleeping for 12 hours..."
            sleep 43200
          done
        volumeMounts:
        - name: scan-results
          mountPath: /tmp
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          capabilities:
            drop:
            - ALL
      - name: security-metrics
        image: prom/node-exporter:v1.6.0
        ports:
        - containerPort: 8090
          name: metrics
        args:
        - '--web.listen-address=:8090'
        - '--path.procfs=/host/proc'
        - '--path.sysfs=/host/sys'
        - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          capabilities:
            drop:
            - ALL
      volumes:
      - name: scan-results
        emptyDir: {}

---
# Security Scanner Service Account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: security-scanner
  namespace: security-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: security-scanner
rules:
- apiGroups: [""]
  resources: ["pods", "nodes", "namespaces"]
  verbs: ["get", "list"]
- apiGroups: ["apps"]
  resources: ["deployments", "daemonsets", "statefulsets"]
  verbs: ["get", "list"]
- apiGroups: ["networking.k8s.io"]
  resources: ["networkpolicies"]
  verbs: ["get", "list"]
- apiGroups: ["policy"]
  resources: ["podsecuritypolicies"]
  verbs: ["get", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: security-scanner
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: security-scanner
subjects:
- kind: ServiceAccount
  name: security-scanner
  namespace: security-system

---
# Security Scanner Service
apiVersion: v1
kind: Service
metadata:
  name: security-scanner
  namespace: security-system
  labels:
    app: security-scanner
spec:
  ports:
  - port: 8090
    targetPort: 8090
    name: metrics
  selector:
    app: security-scanner

---
# Certificate Management - cert-manager ClusterIssuer
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: admin@itdo-erp.com
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    - http01:
        ingress:
          class: nginx
          podTemplate:
            spec:
              nodeSelector:
                "kubernetes.io/os": linux

---
# Admission Controller - OPA Gatekeeper
apiVersion: v1
kind: Namespace
metadata:
  name: gatekeeper-system
  labels:
    admission.gatekeeper.sh/ignore: "no-self-managing"
    control-plane: controller-manager
    gatekeeper.sh/system: "yes"

---
# Gatekeeper ConstraintTemplate - Required Labels
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8srequiredlabels
  annotations:
    description: "Requires all resources to have specified labels"
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredLabels
      validation:
        type: object
        properties:
          labels:
            type: array
            items:
              type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiredlabels
        
        violation[{"msg": msg}] {
          required := input.parameters.labels
          provided := input.review.object.metadata.labels
          missing := required[_]
          not provided[missing]
          msg := sprintf("You must provide labels: %v", [missing])
        }

---
# Gatekeeper Constraint - Enforce Required Labels
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: must-have-app-label
spec:
  match:
    kinds:
      - apiGroups: ["apps"]
        kinds: ["Deployment", "StatefulSet", "DaemonSet"]
    namespaces: ["itdo-erp-prod"]
  parameters:
    labels: ["app", "version"]

---
# Gatekeeper ConstraintTemplate - Container Security
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8scontainersecurity
  annotations:
    description: "Enforces container security best practices"
spec:
  crd:
    spec:
      names:
        kind: K8sContainerSecurity
      validation:
        type: object
        properties:
          allowPrivileged:
            type: boolean
          allowPrivilegeEscalation:
            type: boolean
          requiredDropCapabilities:
            type: array
            items:
              type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8scontainersecurity
        
        violation[{"msg": msg}] {
          container := input.review.object.spec.template.spec.containers[_]
          container.securityContext.privileged == true
          not input.parameters.allowPrivileged
          msg := "Privileged containers are not allowed"
        }
        
        violation[{"msg": msg}] {
          container := input.review.object.spec.template.spec.containers[_]
          container.securityContext.allowPrivilegeEscalation == true
          not input.parameters.allowPrivilegeEscalation
          msg := "Privilege escalation is not allowed"
        }
        
        violation[{"msg": msg}] {
          container := input.review.object.spec.template.spec.containers[_]
          required := input.parameters.requiredDropCapabilities
          provided := container.securityContext.capabilities.drop
          missing := required[_]
          not missing in provided
          msg := sprintf("Container must drop capability: %v", [missing])
        }

---
# Gatekeeper Constraint - Enforce Container Security
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sContainerSecurity
metadata:
  name: container-security-policy
spec:
  match:
    kinds:
      - apiGroups: ["apps"]
        kinds: ["Deployment", "StatefulSet", "DaemonSet"]
    namespaces: ["itdo-erp-prod"]
  parameters:
    allowPrivileged: false
    allowPrivilegeEscalation: false
    requiredDropCapabilities: ["ALL"]

---
# Falco Security Runtime Monitoring
apiVersion: v1
kind: ConfigMap
metadata:
  name: falco-config
  namespace: security-system
data:
  falco.yaml: |
    rules_file:
      - /etc/falco/falco_rules.yaml
      - /etc/falco/falco_rules.local.yaml
      - /etc/falco/k8s_audit_rules.yaml
      - /etc/falco/rules.d
    
    time_format_iso_8601: true
    json_output: true
    json_include_output_property: true
    json_include_tags_property: true
    
    log_stderr: true
    log_syslog: true
    log_level: info
    
    priority: debug
    
    buffered_outputs: false
    
    syscall_event_drops:
      actions:
        - log
        - alert
      rate: 0.03333
      max_burst: 1000
    
    outputs:
      rate: 1
      max_burst: 1000
    
    syslog_output:
      enabled: true
    
    file_output:
      enabled: true
      keep_alive: false
      filename: /var/log/falco.log
    
    stdout_output:
      enabled: true
    
    webserver:
      enabled: true
      listen_port: 8765
      k8s_healthz_endpoint: /healthz
      ssl_enabled: false
    
    grpc:
      enabled: false
      bind_address: "0.0.0.0:5060"
      threadiness: 0
    
    grpc_output:
      enabled: false

  custom_rules.yaml: |
    # Custom ITDO ERP Security Rules
    
    - rule: Unauthorized Process in Container
      desc: Detect unauthorized processes running in containers
      condition: >
        spawned_process and container and
        not proc.name in (python, python3, uvicorn, gunicorn, nginx, postgres, redis-server, node, npm)
      output: >
        Unauthorized process started in container (user=%user.name user_loginuid=%user.loginuid
        command=%proc.cmdline pid=%proc.pid container_id=%container.id container_name=%container.name
        image=%container.image.repository:%container.image.tag)
      priority: WARNING
      tags: [process, container]
    
    - rule: Sensitive File Access
      desc: Detect access to sensitive files
      condition: >
        open_read and
        (fd.name startswith /etc/passwd or
         fd.name startswith /etc/shadow or
         fd.name startswith /root/.ssh or
         fd.name contains id_rsa or
         fd.name contains id_dsa)
      output: >
        Sensitive file opened for reading (user=%user.name user_loginuid=%user.loginuid
        command=%proc.cmdline pid=%proc.pid file=%fd.name container_id=%container.id
        container_name=%container.name image=%container.image.repository:%container.image.tag)
      priority: WARNING
      tags: [filesystem, container]
    
    - rule: Network Connection to Suspicious Port
      desc: Detect network connections to suspicious ports
      condition: >
        inbound_outbound and
        fd.sport in (22, 23, 135, 139, 445, 1433, 3389, 5432, 6379) and
        not proc.name in (ssh, sshd, postgres, redis-server)
      output: >
        Suspicious network connection (user=%user.name user_loginuid=%user.loginuid
        command=%proc.cmdline pid=%proc.pid connection=%fd.name sport=%fd.sport dport=%fd.dport
        container_id=%container.id container_name=%container.name
        image=%container.image.repository:%container.image.tag)
      priority: NOTICE
      tags: [network, container]
    
    - rule: Container Privilege Escalation
      desc: Detect privilege escalation attempts in containers
      condition: >
        spawned_process and container and
        ((proc.name=su and not proc.args contains -c) or
         proc.name=sudo or
         proc.name in (setuid_binaries))
      output: >
        Privilege escalation attempt in container (user=%user.name user_loginuid=%user.loginuid
        command=%proc.cmdline pid=%proc.pid container_id=%container.id container_name=%container.name
        image=%container.image.repository:%container.image.tag)
      priority: CRITICAL
      tags: [process, container, privilege_escalation]

---
# Falco DaemonSet
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: falco
  namespace: security-system
  labels:
    app: falco
spec:
  selector:
    matchLabels:
      app: falco
  template:
    metadata:
      labels:
        app: falco
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8765"
    spec:
      serviceAccountName: falco
      hostNetwork: true
      hostPID: true
      containers:
      - name: falco
        image: falcosecurity/falco:0.35.1
        args:
        - /usr/bin/falco
        - --cri=/run/containerd/containerd.sock
        - --k8s-api
        - --k8s-api-cert=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        - --k8s-api-token=/var/run/secrets/kubernetes.io/serviceaccount/token
        ports:
        - containerPort: 8765
          name: metrics
        securityContext:
          privileged: true
        volumeMounts:
        - mountPath: /host/var/run/docker.sock
          name: docker-socket
          readOnly: true
        - mountPath: /host/run/containerd/containerd.sock
          name: containerd-socket
          readOnly: true
        - mountPath: /host/dev
          name: dev-fs
          readOnly: true
        - mountPath: /host/proc
          name: proc-fs
          readOnly: true
        - mountPath: /host/boot
          name: boot-fs
          readOnly: true
        - mountPath: /host/lib/modules
          name: lib-modules
          readOnly: true
        - mountPath: /host/usr
          name: usr-fs
          readOnly: true
        - mountPath: /host/etc
          name: etc-fs
          readOnly: true
        - mountPath: /etc/falco
          name: falco-config
        resources:
          requests:
            memory: "512Mi"
            cpu: "100m"
          limits:
            memory: "1Gi"
            cpu: "200m"
        env:
        - name: FALCO_K8S_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
      volumes:
      - name: docker-socket
        hostPath:
          path: /var/run/docker.sock
      - name: containerd-socket
        hostPath:
          path: /run/containerd/containerd.sock
      - name: dev-fs
        hostPath:
          path: /dev
      - name: proc-fs
        hostPath:
          path: /proc
      - name: boot-fs
        hostPath:
          path: /boot
      - name: lib-modules
        hostPath:
          path: /lib/modules
      - name: usr-fs
        hostPath:
          path: /usr
      - name: etc-fs
        hostPath:
          path: /etc
      - name: falco-config
        configMap:
          name: falco-config
      tolerations:
      - effect: NoSchedule
        operator: Exists

---
# Falco Service Account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: falco
  namespace: security-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: falco
rules:
- apiGroups: [""]
  resources: ["nodes", "namespaces", "pods", "replicationcontrollers", "services"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["daemonsets", "deployments", "replicasets", "statefulsets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["extensions"]
  resources: ["daemonsets", "deployments", "replicasets"]
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: falco
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: falco
subjects:
- kind: ServiceAccount
  name: falco
  namespace: security-system

---
# Falco Service
apiVersion: v1
kind: Service
metadata:
  name: falco
  namespace: security-system
  labels:
    app: falco
spec:
  ports:
  - port: 8765
    targetPort: 8765
    name: metrics
  selector:
    app: falco

---
# Secrets Management - External Secrets Operator
apiVersion: v1
kind: Secret
metadata:
  name: vault-token
  namespace: security-system
type: Opaque
stringData:
  token: "hvs.EXAMPLE_VAULT_TOKEN"

---
# Secret Scanner CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: secret-scanner
  namespace: security-system
  labels:
    app: secret-scanner
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: secret-scanner
          restartPolicy: OnFailure
          containers:
          - name: secret-scanner
            image: trufflesecurity/trufflehog:latest
            command:
            - sh
            - -c
            - |
              echo "Starting secret scan..."
              trufflehog git https://github.com/itdojp/ITDO_ERP2.git --only-verified --json > /tmp/secret-scan-results.json
              if [ -s /tmp/secret-scan-results.json ]; then
                echo "Secrets found! Alerting..."
                # Send alert logic here
              else
                echo "No secrets found in scan."
              fi
            resources:
              requests:
                memory: "256Mi"
                cpu: "100m"
              limits:
                memory: "512Mi"
                cpu: "200m"
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              runAsNonRoot: true
              runAsUser: 1000
              capabilities:
                drop:
                - ALL

---
# Secret Scanner Service Account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: secret-scanner
  namespace: security-system

---
# Kubernetes Security Benchmarks
apiVersion: batch/v1
kind: CronJob
metadata:
  name: kube-bench
  namespace: security-system
  labels:
    app: kube-bench
spec:
  schedule: "0 6 * * 0"  # Weekly on Sunday at 6 AM
  jobTemplate:
    spec:
      template:
        spec:
          hostPID: true
          containers:
          - name: kube-bench
            image: aquasec/kube-bench:v0.6.10
            command: ["kube-bench"]
            args: ["--json"]
            volumeMounts:
            - name: var-lib-etcd
              mountPath: /var/lib/etcd
              readOnly: true
            - name: var-lib-kubelet
              mountPath: /var/lib/kubelet
              readOnly: true
            - name: etc-systemd
              mountPath: /etc/systemd
              readOnly: true
            - name: etc-kubernetes
              mountPath: /etc/kubernetes
              readOnly: true
            - name: usr-bin
              mountPath: /usr/local/mount-from-host/bin
              readOnly: true
            resources:
              requests:
                memory: "256Mi"
                cpu: "100m"
              limits:
                memory: "512Mi"
                cpu: "200m"
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              runAsNonRoot: true
              runAsUser: 1000
              capabilities:
                drop:
                - ALL
          restartPolicy: OnFailure
          volumes:
          - name: var-lib-etcd
            hostPath:
              path: "/var/lib/etcd"
          - name: var-lib-kubelet
            hostPath:
              path: "/var/lib/kubelet"
          - name: etc-systemd
            hostPath:
              path: "/etc/systemd"
          - name: etc-kubernetes
            hostPath:
              path: "/etc/kubernetes"
          - name: usr-bin
            hostPath:
              path: "/usr/bin"

---
# Security Dashboard Service
apiVersion: v1
kind: Service
metadata:
  name: security-dashboard
  namespace: security-system
  labels:
    app: security-dashboard
spec:
  ports:
  - port: 3000
    targetPort: 3000
    name: web
  selector:
    app: security-dashboard

---
# Security Dashboard Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: security-dashboard
  namespace: security-system
  labels:
    app: security-dashboard
spec:
  replicas: 1
  selector:
    matchLabels:
      app: security-dashboard
  template:
    metadata:
      labels:
        app: security-dashboard
    spec:
      serviceAccountName: security-dashboard
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
      containers:
      - name: dashboard
        image: grafana/grafana:10.0.0
        ports:
        - containerPort: 3000
          name: web
        env:
        - name: GF_SECURITY_ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              name: security-dashboard-secret
              key: admin-password
        - name: GF_INSTALL_PLUGINS
          value: "grafana-piechart-panel,grafana-worldmap-panel"
        volumeMounts:
        - name: grafana-storage
          mountPath: /var/lib/grafana
        - name: grafana-config
          mountPath: /etc/grafana
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
          capabilities:
            drop:
            - ALL
        livenessProbe:
          httpGet:
            path: /api/health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /api/health
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: grafana-storage
        persistentVolumeClaim:
          claimName: security-dashboard-storage
      - name: grafana-config
        configMap:
          name: security-dashboard-config

---
# Security Dashboard PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: security-dashboard-storage
  namespace: security-system
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: fast-ssd

---
# Security Dashboard Service Account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: security-dashboard
  namespace: security-system

---
# Security Dashboard Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: security-dashboard-config
  namespace: security-system
data:
  grafana.ini: |
    [analytics]
    check_for_updates = false
    
    [grafana_net]
    url = https://grafana.net
    
    [log]
    mode = console
    level = info
    
    [paths]
    data = /var/lib/grafana/data
    logs = /var/log/grafana
    plugins = /var/lib/grafana/plugins
    provisioning = /etc/grafana/provisioning
    
    [server]
    http_port = 3000
    root_url = https://security.itdo-erp.com
    
    [database]
    type = sqlite3
    path = grafana.db
    
    [security]
    admin_user = admin
    admin_password = $__env{GF_SECURITY_ADMIN_PASSWORD}
    secret_key = security-dashboard-secret-key
    disable_gravatar = true
    
    [auth]
    disable_login_form = false
    disable_signout_menu = false
    
    [users]
    allow_sign_up = false
    allow_org_create = false
    auto_assign_org = true
    auto_assign_org_role = Viewer
    
    [snapshots]
    external_enabled = false

---
# Ingress for Security Dashboard
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: security-dashboard-ingress
  namespace: security-system
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: security-auth
    nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required - Security Dashboard'
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - security.itdo-erp.com
    secretName: security-dashboard-tls
  rules:
  - host: security.itdo-erp.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: security-dashboard
            port:
              number: 3000