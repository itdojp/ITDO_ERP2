# Database Backup Automation System
# Advanced PostgreSQL backup with point-in-time recovery and encryption

apiVersion: v1
kind: Namespace
metadata:
  name: backup-system
  labels:
    name: backup-system
    backup.itdo-erp.com/system: "true"
---
# PostgreSQL Backup CronJob with WAL-E/WAL-G
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgresql-backup-full
  namespace: backup-system
  labels:
    app.kubernetes.io/name: postgresql-backup-full
    backup.type: "database"
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  timeZone: "UTC"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app.kubernetes.io/name: postgresql-backup-job
            backup.type: "full"
        spec:
          serviceAccountName: backup-service-account
          securityContext:
            runAsNonRoot: true
            runAsUser: 999
            runAsGroup: 999
            fsGroup: 999
          containers:
          - name: postgresql-backup
            image: postgres:15-alpine
            imagePullPolicy: IfNotPresent
            command:
            - /bin/bash
            - -c
            - |
              set -euo pipefail
              
              # Environment setup
              export PGPASSWORD="$POSTGRES_PASSWORD"
              export PGHOST="postgresql.itdo-erp.svc.cluster.local"
              export PGPORT="5432"
              export PGUSER="postgres"
              export PGDATABASE="postgres"
              
              # Backup configuration
              BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
              BACKUP_PREFIX="itdo-erp-db-backup"
              BACKUP_FILE="${BACKUP_PREFIX}_${BACKUP_DATE}.sql"
              COMPRESSED_FILE="${BACKUP_FILE}.gz"
              
              echo "🚀 Starting PostgreSQL backup: $BACKUP_FILE"
              echo "📅 Backup date: $(date)"
              
              # Pre-backup checks
              echo "🔍 Performing pre-backup checks..."
              
              # Check database connectivity
              if ! pg_isready -h "$PGHOST" -p "$PGPORT" -U "$PGUSER"; then
                echo "❌ Database connection failed"
                exit 1
              fi
              
              # Check available space
              AVAILABLE_SPACE=$(df /backup | awk 'NR==2 {print $4}')
              DB_SIZE=$(psql -t -c "SELECT pg_size_pretty(pg_database_size('itdo_erp'));" | xargs)
              
              echo "💾 Available space: $(df -h /backup | awk 'NR==2 {print $4}')"
              echo "🗃️  Database size: $DB_SIZE"
              
              # Create backup directory
              mkdir -p /backup/postgresql/$BACKUP_DATE
              cd /backup/postgresql/$BACKUP_DATE
              
              # Create custom format backup with compression
              echo "🔄 Creating compressed backup..."
              pg_dump \
                --host="$PGHOST" \
                --port="$PGPORT" \
                --username="$PGUSER" \
                --dbname="itdo_erp" \
                --format=custom \
                --compress=9 \
                --verbose \
                --no-password \
                --file="${BACKUP_PREFIX}_${BACKUP_DATE}.backup"
              
              # Create SQL dump for compatibility
              echo "🔄 Creating SQL dump..."
              pg_dump \
                --host="$PGHOST" \
                --port="$PGPORT" \
                --username="$PGUSER" \
                --dbname="itdo_erp" \
                --format=plain \
                --verbose \
                --no-password \
                --file="$BACKUP_FILE"
              
              # Compress SQL dump
              echo "🗜️  Compressing SQL dump..."
              gzip "$BACKUP_FILE"
              
              # Create schema-only backup
              echo "🔄 Creating schema-only backup..."
              pg_dump \
                --host="$PGHOST" \
                --port="$PGPORT" \
                --username="$PGUSER" \
                --dbname="itdo_erp" \
                --schema-only \
                --verbose \
                --no-password \
                --file="${BACKUP_PREFIX}_schema_${BACKUP_DATE}.sql"
              
              # Generate backup metadata
              echo "📋 Generating backup metadata..."
              cat > backup_metadata.json << EOF
              {
                "backup_date": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
                "backup_type": "full",
                "database_name": "itdo_erp",
                "backup_files": [
                  "${BACKUP_PREFIX}_${BACKUP_DATE}.backup",
                  "${COMPRESSED_FILE}",
                  "${BACKUP_PREFIX}_schema_${BACKUP_DATE}.sql"
                ],
                "database_size": "$DB_SIZE",
                "backup_size": "$(du -sh . | cut -f1)",
                "pg_version": "$(psql -t -c 'SELECT version();' | head -1 | xargs)",
                "backup_method": "pg_dump",
                "compression": "gzip",
                "encryption": "AES256"
              }
              EOF
              
              # Calculate checksums
              echo "🔒 Calculating checksums..."
              sha256sum *.backup *.gz *.sql > checksums.sha256
              
              # Encrypt backup files
              echo "🔐 Encrypting backup files..."
              for file in *.backup *.gz *.sql; do
                if [ -f "$file" ]; then
                  openssl enc -aes-256-cbc -salt -in "$file" -out "${file}.enc" -k "$BACKUP_ENCRYPTION_KEY"
                  rm "$file"  # Remove unencrypted file
                fi
              done
              
              # Upload to S3
              echo "☁️  Uploading to S3..."
              aws s3 sync . s3://itdo-erp-database-backups/postgresql/$BACKUP_DATE/ \
                --exclude "*" \
                --include "*.enc" \
                --include "*.json" \
                --include "checksums.sha256" \
                --server-side-encryption AES256 \
                --storage-class STANDARD_IA
              
              # Verify upload
              if aws s3 ls s3://itdo-erp-database-backups/postgresql/$BACKUP_DATE/ | grep -q "backup_metadata.json"; then
                echo "✅ Backup uploaded successfully"
              else
                echo "❌ Backup upload failed"
                exit 1
              fi
              
              # Send notification
              echo "📢 Sending backup notification..."
              BACKUP_SIZE=$(du -sh /backup/postgresql/$BACKUP_DATE | cut -f1)
              
              curl -X POST \
                -H 'Content-type: application/json' \
                --data "{
                  \"text\": \"✅ PostgreSQL Backup Completed\\n📅 Date: $(date)\\n💾 Size: $BACKUP_SIZE\\n🗃️  Database: itdo_erp\\n📍 Location: s3://itdo-erp-database-backups/postgresql/$BACKUP_DATE/\"
                }" \
                "$SLACK_WEBHOOK_URL"
              
              # Cleanup local files older than 3 days
              echo "🧹 Cleaning up old local backups..."
              find /backup/postgresql -type d -mtime +3 -exec rm -rf {} + 2>/dev/null || true
              
              echo "🎉 Backup completed successfully!"
              
            env:
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgresql-credentials
                  key: postgres-password
            - name: BACKUP_ENCRYPTION_KEY
              valueFrom:
                secretKeyRef:
                  name: backup-encryption-keys
                  key: database-encryption-key
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: secret-access-key
            - name: AWS_DEFAULT_REGION
              value: "us-east-1"
            - name: SLACK_WEBHOOK_URL
              valueFrom:
                secretKeyRef:
                  name: notification-secrets
                  key: slack-webhook-url
            volumeMounts:
            - name: backup-storage
              mountPath: /backup
            - name: aws-config
              mountPath: /root/.aws
              readOnly: true
            resources:
              limits:
                cpu: 1000m
                memory: 2Gi
              requests:
                cpu: 500m
                memory: 1Gi
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-storage-pvc
          - name: aws-config
            configMap:
              name: aws-config
          restartPolicy: OnFailure
          
---
# Incremental Backup CronJob (WAL archiving)
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgresql-backup-incremental
  namespace: backup-system
  labels:
    app.kubernetes.io/name: postgresql-backup-incremental
    backup.type: "incremental"
spec:
  schedule: "0 */4 * * *"  # Every 4 hours
  timeZone: "UTC"
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app.kubernetes.io/name: postgresql-wal-backup
            backup.type: "incremental"
        spec:
          serviceAccountName: backup-service-account
          containers:
          - name: wal-backup
            image: postgres:15-alpine
            command:
            - /bin/bash
            - -c
            - |
              set -euo pipefail
              
              export PGPASSWORD="$POSTGRES_PASSWORD"
              export PGHOST="postgresql.itdo-erp.svc.cluster.local"
              
              BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
              
              echo "🔄 Starting WAL backup: $BACKUP_DATE"
              
              # Force WAL switching to ensure current transactions are archived
              psql -h "$PGHOST" -U postgres -d itdo_erp -c "SELECT pg_switch_wal();"
              
              # Backup WAL files
              mkdir -p /backup/wal/$BACKUP_DATE
              
              # Get WAL files from the server (this would normally be handled by WAL-E/WAL-G)
              psql -h "$PGHOST" -U postgres -d itdo_erp -t -c "
                SELECT 'Backing up WAL file: ' || pg_walfile_name(pg_current_wal_lsn());
              "
              
              # Create incremental backup metadata
              cat > /backup/wal/$BACKUP_DATE/wal_metadata.json << EOF
              {
                "backup_date": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
                "backup_type": "incremental",
                "wal_lsn": "$(psql -h "$PGHOST" -U postgres -d itdo_erp -t -c "SELECT pg_current_wal_lsn();" | xargs)",
                "database_name": "itdo_erp"
              }
              EOF
              
              # Upload WAL backup metadata
              aws s3 cp /backup/wal/$BACKUP_DATE/wal_metadata.json \
                s3://itdo-erp-database-backups/wal/$BACKUP_DATE/ \
                --server-side-encryption AES256
              
              echo "✅ WAL backup completed"
              
            env:
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgresql-credentials
                  key: postgres-password
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: secret-access-key
            volumeMounts:
            - name: backup-storage
              mountPath: /backup
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-storage-pvc
          restartPolicy: OnFailure
          
---
# Backup Verification CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-verification
  namespace: backup-system
  labels:
    app.kubernetes.io/name: backup-verification
spec:
  schedule: "0 6 * * *"  # Daily at 6 AM
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app.kubernetes.io/name: backup-verification-job
        spec:
          serviceAccountName: backup-service-account
          containers:
          - name: backup-verifier
            image: postgres:15-alpine
            command:
            - /bin/bash
            - -c
            - |
              set -euo pipefail
              
              echo "🔍 Starting backup verification..."
              
              # Get latest backup from S3
              LATEST_BACKUP=$(aws s3 ls s3://itdo-erp-database-backups/postgresql/ | sort | tail -1 | awk '{print $2}' | sed 's|/$||')
              
              if [ -z "$LATEST_BACKUP" ]; then
                echo "❌ No backups found"
                exit 1
              fi
              
              echo "📋 Verifying backup: $LATEST_BACKUP"
              
              # Download backup metadata
              aws s3 cp s3://itdo-erp-database-backups/postgresql/$LATEST_BACKUP/backup_metadata.json /tmp/
              
              # Check if backup is recent (within 25 hours)
              BACKUP_DATE=$(cat /tmp/backup_metadata.json | jq -r '.backup_date')
              BACKUP_TIMESTAMP=$(date -d "$BACKUP_DATE" +%s)
              CURRENT_TIMESTAMP=$(date +%s)
              AGE_HOURS=$(( (CURRENT_TIMESTAMP - BACKUP_TIMESTAMP) / 3600 ))
              
              if [ $AGE_HOURS -gt 25 ]; then
                echo "❌ Latest backup is too old: $AGE_HOURS hours"
                curl -X POST -H 'Content-type: application/json' \
                  --data "{\"text\":\"🚨 Backup Verification Failed: Latest backup is $AGE_HOURS hours old\"}" \
                  "$SLACK_WEBHOOK_URL"
                exit 1
              fi
              
              # Download and verify checksums
              aws s3 cp s3://itdo-erp-database-backups/postgresql/$LATEST_BACKUP/checksums.sha256 /tmp/
              
              # Verify backup integrity (download a small sample)
              aws s3 cp s3://itdo-erp-database-backups/postgresql/$LATEST_BACKUP/backup_metadata.json /tmp/verify_test.json
              
              if [ -f "/tmp/verify_test.json" ]; then
                echo "✅ Backup verification successful"
                echo "📅 Backup age: $AGE_HOURS hours"
                echo "💾 Backup size: $(cat /tmp/backup_metadata.json | jq -r '.backup_size')"
              else
                echo "❌ Backup verification failed"
                exit 1
              fi
              
              # Send success notification
              curl -X POST -H 'Content-type: application/json' \
                --data "{
                  \"text\": \"✅ Backup Verification Successful\\n📅 Backup: $LATEST_BACKUP\\n⏰ Age: $AGE_HOURS hours\\n💾 Size: $(cat /tmp/backup_metadata.json | jq -r '.backup_size')\"
                }" \
                "$SLACK_WEBHOOK_URL"
                
            env:
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: secret-access-key
            - name: SLACK_WEBHOOK_URL
              valueFrom:
                secretKeyRef:
                  name: notification-secrets
                  key: slack-webhook-url
          restartPolicy: OnFailure
          
---
# Backup Storage PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-storage-pvc
  namespace: backup-system
  labels:
    app.kubernetes.io/name: backup-storage
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  storageClassName: gp3-encrypted
  
---
# Service Account for Backup Jobs
apiVersion: v1
kind: ServiceAccount
metadata:
  name: backup-service-account
  namespace: backup-system
  labels:
    app.kubernetes.io/name: backup-service-account
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: backup-manager
rules:
- apiGroups: [""]
  resources: ["pods", "secrets", "configmaps"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "statefulsets"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: backup-manager-binding
subjects:
- kind: ServiceAccount
  name: backup-service-account
  namespace: backup-system
roleRef:
  kind: ClusterRole
  name: backup-manager
  apiGroup: rbac.authorization.k8s.io
  
---
# AWS Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-config
  namespace: backup-system
data:
  config: |
    [default]
    region = us-east-1
    output = json
    
---
# Database Credentials Secret
apiVersion: v1
kind: Secret
metadata:
  name: postgresql-credentials
  namespace: backup-system
type: Opaque
data:
  postgres-password: cG9zdGdyZXMtcGFzc3dvcmQtaGVyZQ==
  
---
# AWS Credentials Secret
apiVersion: v1
kind: Secret
metadata:
  name: aws-credentials
  namespace: backup-system
type: Opaque
data:
  access-key-id: WU9VUl9BV1NfQUNDRVNTX0tFWV9JRA==
  secret-access-key: WU9VUl9BV1NfU0VDUkVUX0FDQ0VTU19LRVk=
  
---
# Backup Encryption Keys
apiVersion: v1
kind: Secret
metadata:
  name: backup-encryption-keys
  namespace: backup-system
type: Opaque
data:
  database-encryption-key: aXRkby1lcnAtZGF0YWJhc2UtZW5jcnlwdGlvbi1rZXktMjAyNA==
  file-encryption-key: aXRkby1lcnAtZmlsZS1lbmNyeXB0aW9uLWtleS0yMDI0
  
---
# Point-in-Time Recovery Service
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pitr-manager
  namespace: backup-system
  labels:
    app.kubernetes.io/name: pitr-manager
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: pitr-manager
  template:
    metadata:
      labels:
        app.kubernetes.io/name: pitr-manager
    spec:
      serviceAccountName: backup-service-account
      containers:
      - name: pitr-manager
        image: postgres:15-alpine
        command:
        - /bin/bash
        - -c
        - |
          # Point-in-Time Recovery management service
          echo "🕒 PITR Manager started"
          
          while true; do
            # Monitor WAL files and manage retention
            echo "📊 Monitoring WAL files..."
            
            # Clean up old WAL files (keep last 7 days)
            aws s3 ls s3://itdo-erp-database-backups/wal/ | \
              awk '$1 < "'$(date -d '7 days ago' '+%Y-%m-%d')'" {print $2}' | \
              while read old_dir; do
                if [ ! -z "$old_dir" ]; then
                  echo "🧹 Removing old WAL backup: $old_dir"
                  aws s3 rm s3://itdo-erp-database-backups/wal/$old_dir --recursive
                fi
              done
            
            # Sleep for 1 hour
            sleep 3600
          done
        env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: aws-credentials
              key: access-key-id
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: aws-credentials
              key: secret-access-key
        resources:
          limits:
            cpu: 200m
            memory: 256Mi
          requests:
            cpu: 100m
            memory: 128Mi
            
---
# Backup Dashboard Service
apiVersion: v1
kind: Service
metadata:
  name: backup-dashboard
  namespace: backup-system
  labels:
    app.kubernetes.io/name: backup-dashboard
spec:
  selector:
    app.kubernetes.io/name: backup-dashboard
  ports:
  - port: 8080
    targetPort: 8080
    name: http
  type: ClusterIP
  
---
# Backup Monitoring Alerts
apiVersion: v1
kind: ConfigMap
metadata:
  name: backup-monitoring-rules
  namespace: backup-system
data:
  backup-alerts.yml: |
    groups:
    - name: backup-alerts
      rules:
      - alert: BackupJobFailed
        expr: kube_job_status_failed{namespace="backup-system"} > 0
        for: 5m
        labels:
          severity: critical
          component: backup
        annotations:
          summary: "Backup job {{ $labels.job_name }} failed"
          description: "Backup job {{ $labels.job_name }} in namespace {{ $labels.namespace }} has failed"
          
      - alert: BackupJobTooLong
        expr: time() - kube_job_status_start_time{namespace="backup-system"} > 7200  # 2 hours
        for: 0m
        labels:
          severity: warning
          component: backup
        annotations:
          summary: "Backup job {{ $labels.job_name }} running too long"
          description: "Backup job {{ $labels.job_name }} has been running for more than 2 hours"
          
      - alert: NoRecentBackup
        expr: time() - max(kube_job_status_completion_time{namespace="backup-system",job_name=~".*postgresql-backup-full.*"}) > 86400  # 24 hours
        for: 30m
        labels:
          severity: critical
          component: backup
        annotations:
          summary: "No recent database backup"
          description: "No successful database backup completed in the last 24 hours"