# Multi-Region Disaster Recovery System
# Enterprise-grade DR with automatic failover and data synchronization

apiVersion: v1
kind: Namespace
metadata:
  name: disaster-recovery
  labels:
    name: disaster-recovery
    dr.itdo-erp.com/system: "true"
    
---
# ArgoCD Application for DR Site Management
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: itdo-erp-dr-primary
  namespace: argocd
  labels:
    app.kubernetes.io/name: itdo-erp-dr-primary
    dr.region: "us-east-1"
spec:
  project: default
  source:
    repoURL: https://github.com/itdo-erp/infrastructure
    targetRevision: main
    path: infra/disaster-recovery/clusters/primary
  destination:
    server: https://kubernetes.default.svc
    namespace: itdo-erp
  syncPolicy:
    automated:
      prune: false
      selfHeal: true
      allowEmpty: false
    syncOptions:
    - CreateNamespace=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
        
---
# DR Site Application (Secondary Region)
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: itdo-erp-dr-secondary
  namespace: argocd
  labels:
    app.kubernetes.io/name: itdo-erp-dr-secondary
    dr.region: "us-west-2"
spec:
  project: default
  source:
    repoURL: https://github.com/itdo-erp/infrastructure
    targetRevision: main
    path: infra/disaster-recovery/clusters/secondary
  destination:
    server: https://dr-cluster.us-west-2.eks.amazonaws.com
    namespace: itdo-erp
  syncPolicy:
    automated:
      prune: false
      selfHeal: false  # Manual sync for DR site
      allowEmpty: false
    syncOptions:
    - CreateNamespace=true
    
---
# External-DNS Configuration for Multi-Region
apiVersion: v1
kind: ConfigMap
metadata:
  name: external-dns-dr-config
  namespace: disaster-recovery
  labels:
    app.kubernetes.io/name: external-dns-dr
data:
  external-dns.yml: |
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: external-dns-dr
      namespace: kube-system
    spec:
      template:
        spec:
          containers:
          - name: external-dns
            image: k8s.gcr.io/external-dns/external-dns:v0.13.5
            args:
            - --source=service
            - --source=ingress
            - --domain-filter=itdo-erp.com
            - --provider=aws
            - --aws-zone-type=public
            - --aws-assume-role=arn:aws:iam::123456789012:role/external-dns-dr
            - --policy=sync
            - --registry=txt
            - --txt-owner-id=itdo-erp-dr-cluster
            - --txt-prefix=itdo-erp-dr-
            - --interval=1m
            - --log-level=info
            - --events
            # Health check configuration
            - --aws-health-check-id=Z123456789012345678901
            - --aws-health-check-type=HTTPS
            - --aws-health-check-resource-path=/health
            - --aws-health-check-interval=30
            - --aws-health-check-failure-threshold=3
            
---
# Database Replication Configuration
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgresql-dr-replica
  namespace: disaster-recovery
  labels:
    app.kubernetes.io/name: postgresql-dr-replica
    dr.role: "replica"
spec:
  instances: 3
  
  postgresql:
    parameters:
      # Streaming replication settings
      wal_level: "replica"
      max_wal_senders: "10"
      max_replication_slots: "10"
      hot_standby: "on"
      hot_standby_feedback: "on"
      
      # Recovery settings
      recovery_target_timeline: "latest"
      standby_mode: "on"
      primary_conninfo: "host=postgresql-primary.us-east-1.rds.amazonaws.com port=5432 user=replicator password=REPLICA_PASSWORD sslmode=require application_name=dr_replica"
      
      # Archive recovery
      archive_mode: "on"
      archive_command: "wal-g wal-push %p"
      restore_command: "wal-g wal-fetch %f %p"
      
      # Monitoring and logging
      log_statement: "all"
      log_replication_commands: "on"
      log_min_duration_statement: "1000"
      
  bootstrap:
    recovery:
      source: "primary-cluster"
      
  externalClusters:
  - name: "primary-cluster"
    connectionParameters:
      host: "postgresql-primary.us-east-1.rds.amazonaws.com"
      user: "postgres"
      dbname: "itdo_erp"
      sslmode: "require"
    password:
      name: "postgresql-primary-credentials"
      key: "password"
      
  # Storage configuration for DR
  storage:
    size: "1Ti"
    storageClass: "gp3-encrypted"
    
  # Monitoring
  monitoring:
    enabled: true
    
---
# Redis Replication for Session Store DR
apiVersion: redis.redis.opstreelabs.in/v1beta1
kind: RedisReplication
metadata:
  name: redis-dr-replica
  namespace: disaster-recovery
  labels:
    app.kubernetes.io/name: redis-dr-replica
    dr.role: "replica"
spec:
  clusterSize: 3
  
  redisConfig:
    # Replication configuration
    replicaof: "redis-primary.us-east-1.elasticache.amazonaws.com 6379"
    replica-read-only: "yes"
    replica-serve-stale-data: "yes"
    
    # Security
    requirepass: "REDIS_PASSWORD"
    masterauth: "REDIS_PRIMARY_PASSWORD"
    
    # Persistence
    save: "900 1 300 10 60 10000"
    rdbcompression: "yes"
    rdbchecksum: "yes"
    
    # AOF
    appendonly: "yes"
    appendfsync: "everysec"
    
  storage:
    volumeClaimTemplate:
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: "50Gi"
        storageClassName: "gp3-encrypted"
        
---
# DR Failover Controller
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dr-failover-controller
  namespace: disaster-recovery
  labels:
    app.kubernetes.io/name: dr-failover-controller
spec:
  replicas: 2  # Active-passive
  selector:
    matchLabels:
      app.kubernetes.io/name: dr-failover-controller
  template:
    metadata:
      labels:
        app.kubernetes.io/name: dr-failover-controller
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
    spec:
      serviceAccountName: dr-failover-controller
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
      containers:
      - name: failover-controller
        image: itdo-erp/dr-failover-controller:v1.0.0
        imagePullPolicy: IfNotPresent
        ports:
        - name: http-metrics
          containerPort: 8080
        - name: webhook
          containerPort: 9443
        command:
        - /manager
        args:
        - --health-probe-bind-address=:8081
        - --metrics-bind-address=:8080
        - --leader-elect
        - --primary-region=us-east-1
        - --dr-region=us-west-2
        - --health-check-interval=30s
        - --failover-threshold=3
        - --rpo-threshold=300  # 5 minutes RPO
        - --rto-target=900     # 15 minutes RTO
        env:
        - name: PRIMARY_CLUSTER_ENDPOINT
          value: "https://kubernetes.us-east-1.amazonaws.com"
        - name: DR_CLUSTER_ENDPOINT
          value: "https://kubernetes.us-west-2.amazonaws.com"
        - name: DNS_ZONE_ID
          value: "Z1D633PJN98FT9"
        - name: SLACK_WEBHOOK_URL
          valueFrom:
            secretKeyRef:
              name: notification-secrets
              key: slack-webhook-url
        - name: PAGERDUTY_INTEGRATION_KEY
          valueFrom:
            secretKeyRef:
              name: notification-secrets
              key: pagerduty-integration-key
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8081
          initialDelaySeconds: 15
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8081
          initialDelaySeconds: 5
          periodSeconds: 10
        resources:
          limits:
            cpu: 500m
            memory: 256Mi
          requests:
            cpu: 100m
            memory: 128Mi
        volumeMounts:
        - name: webhook-certs
          mountPath: /tmp/k8s-webhook-server/serving-certs
          readOnly: true
        - name: aws-credentials
          mountPath: /root/.aws
          readOnly: true
      volumes:
      - name: webhook-certs
        secret:
          secretName: webhook-server-certs
      - name: aws-credentials
        secret:
          secretName: aws-credentials
          
---
# DR Service Account and RBAC
apiVersion: v1
kind: ServiceAccount
metadata:
  name: dr-failover-controller
  namespace: disaster-recovery
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/ITDOERPDRController
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: dr-failover-controller
rules:
- apiGroups: [""]
  resources: ["services", "endpoints", "configmaps", "secrets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments", "statefulsets", "daemonsets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["networking.k8s.io"]
  resources: ["ingresses", "networkpolicies"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["postgresql.cnpg.io"]
  resources: ["clusters"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["redis.redis.opstreelabs.in"]
  resources: ["redisreplications"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["argoproj.io"]
  resources: ["applications"]
  verbs: ["get", "list", "watch", "update", "patch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: dr-failover-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: dr-failover-controller
subjects:
- kind: ServiceAccount
  name: dr-failover-controller
  namespace: disaster-recovery
  
---
# DR Health Monitoring Service
apiVersion: v1
kind: Service
metadata:
  name: dr-health-monitor
  namespace: disaster-recovery
  labels:
    app.kubernetes.io/name: dr-health-monitor
spec:
  selector:
    app.kubernetes.io/name: dr-health-monitor
  ports:
  - port: 8080
    targetPort: 8080
    name: http
  type: ClusterIP
  
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dr-health-monitor
  namespace: disaster-recovery
  labels:
    app.kubernetes.io/name: dr-health-monitor
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: dr-health-monitor
  template:
    metadata:
      labels:
        app.kubernetes.io/name: dr-health-monitor
    spec:
      serviceAccountName: dr-failover-controller
      containers:
      - name: health-monitor
        image: curlimages/curl:8.2.0
        command:
        - /bin/sh
        - -c
        - |
          cat > /app/health-monitor.sh << 'EOF'
          #!/bin/sh
          set -e
          
          PRIMARY_HEALTH_URL="https://api.itdo-erp.com/health"
          DR_HEALTH_URL="https://dr.api.itdo-erp.com/health"
          DATABASE_HEALTH_URL="https://api.itdo-erp.com/health/database"
          
          check_endpoint() {
            local url=$1
            local name=$2
            
            if curl -s -f -m 10 "$url" > /dev/null; then
              echo "‚úÖ $name is healthy"
              return 0
            else
              echo "‚ùå $name is unhealthy"
              return 1
            fi
          }
          
          check_database_lag() {
            # Check replication lag (this would be implemented based on your DB setup)
            local lag=$(curl -s "https://api.itdo-erp.com/metrics/database/replication_lag" | grep -o '[0-9]*')
            
            if [ "$lag" -lt 60 ]; then  # Less than 60 seconds lag
              echo "‚úÖ Database replication lag: ${lag}s"
              return 0
            else
              echo "‚ö†Ô∏è  Database replication lag: ${lag}s (high)"
              return 1
            fi
          }
          
          send_alert() {
            local message="$1"
            local severity="$2"
            
            # Send Slack notification
            if [ ! -z "$SLACK_WEBHOOK_URL" ]; then
              curl -X POST -H 'Content-type: application/json' \
                --data "{\"text\":\"$severity $message\"}" \
                "$SLACK_WEBHOOK_URL"
            fi
            
            # Send PagerDuty alert for critical issues
            if [ "$severity" = "üö®" ] && [ ! -z "$PAGERDUTY_INTEGRATION_KEY" ]; then
              curl -X POST \
                -H "Content-Type: application/json" \
                -H "Authorization: Token token=$PAGERDUTY_INTEGRATION_KEY" \
                --data "{
                  \"incident\": {
                    \"type\": \"incident\",
                    \"title\": \"DR Health Check Failed: $message\",
                    \"service\": {\"id\": \"PSERVICE1\", \"type\": \"service_reference\"},
                    \"urgency\": \"high\",
                    \"body\": {\"type\": \"incident_body\", \"details\": \"$message\"}
                  }
                }" \
                "https://api.pagerduty.com/incidents"
            fi
          }
          
          # Main monitoring loop
          while true; do
            echo "üîç Starting DR health check - $(date)"
            
            # Check primary site
            if ! check_endpoint "$PRIMARY_HEALTH_URL" "Primary Site"; then
              send_alert "Primary site health check failed" "üö®"
              
              # Check if DR site is healthy for potential failover
              if check_endpoint "$DR_HEALTH_URL" "DR Site"; then
                send_alert "Primary site down but DR site is healthy - consider failover" "‚ö†Ô∏è"
              else
                send_alert "Both primary and DR sites are unhealthy!" "üö®"
              fi
            fi
            
            # Check DR site readiness
            if ! check_endpoint "$DR_HEALTH_URL" "DR Site"; then
              send_alert "DR site health check failed" "‚ö†Ô∏è"
            fi
            
            # Check database replication
            if ! check_database_lag; then
              send_alert "Database replication lag is high" "‚ö†Ô∏è"
            fi
            
            # Check Route 53 health checks
            aws route53 list-health-checks --query 'HealthChecks[?Config.Type==`HTTPS`].{Id:Id,Status:StatusList.Status}' \
              --output text | while read id status; do
              if [ "$status" != "Success" ]; then
                send_alert "Route 53 health check $id failed with status: $status" "‚ö†Ô∏è"
              fi
            done
            
            echo "‚úÖ DR health check completed - $(date)"
            echo "---"
            
            # Sleep for 30 seconds
            sleep 30
          done
          EOF
          
          chmod +x /app/health-monitor.sh
          exec /app/health-monitor.sh
        env:
        - name: SLACK_WEBHOOK_URL
          valueFrom:
            secretKeyRef:
              name: notification-secrets
              key: slack-webhook-url
        - name: PAGERDUTY_INTEGRATION_KEY
          valueFrom:
            secretKeyRef:
              name: notification-secrets
              key: pagerduty-integration-key
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: aws-credentials
              key: access-key-id
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: aws-credentials
              key: secret-access-key
        volumeMounts:
        - name: app-dir
          mountPath: /app
        resources:
          limits:
            cpu: 100m
            memory: 128Mi
          requests:
            cpu: 50m
            memory: 64Mi
      volumes:
      - name: app-dir
        emptyDir: {}
        
---
# DR Runbook ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: dr-runbook
  namespace: disaster-recovery
  labels:
    app.kubernetes.io/name: dr-runbook
data:
  failover-procedure.md: |
    # ITDO ERP Disaster Recovery Failover Procedure
    
    ## üö® Emergency Failover Steps
    
    ### 1. Assess Situation (RTO: 5 minutes)
    - [ ] Confirm primary site is completely unavailable
    - [ ] Check database replication lag < 5 minutes
    - [ ] Verify DR site health status
    - [ ] Notify stakeholders of impending failover
    
    ### 2. Database Failover (RTO: 5 minutes)
    ```bash
    # Promote replica to primary
    kubectl -n disaster-recovery patch cluster postgresql-dr-replica \
      --type='merge' -p='{"spec":{"bootstrap":{"recovery":null}}}'
    
    # Wait for promotion
    kubectl wait --for=condition=Ready cluster/postgresql-dr-replica \
      --timeout=300s -n disaster-recovery
    ```
    
    ### 3. Application Failover (RTO: 3 minutes)
    ```bash
    # Scale up DR applications
    kubectl scale deployment itdo-erp-backend-api --replicas=3 \
      -n itdo-erp --context=dr-cluster
    kubectl scale deployment itdo-erp-frontend-app --replicas=3 \
      -n itdo-erp --context=dr-cluster
    
    # Update DNS to point to DR site
    aws route53 change-resource-record-sets \
      --hosted-zone-id Z1D633PJN98FT9 \
      --change-batch file://dr-dns-failover.json
    ```
    
    ### 4. Verification (RTO: 2 minutes)
    ```bash
    # Test application endpoints
    curl -f https://api.itdo-erp.com/health
    curl -f https://app.itdo-erp.com/health
    
    # Check database connectivity
    psql -h postgresql-dr-replica.disaster-recovery.svc.cluster.local \
      -U postgres -d itdo_erp -c "SELECT NOW();"
    ```
    
    ## üîÑ Planned Failover Steps
    
    ### 1. Pre-failover Preparation
    - [ ] Schedule maintenance window
    - [ ] Notify all users and stakeholders
    - [ ] Ensure data synchronization is current
    - [ ] Verify DR site readiness
    
    ### 2. Graceful Failover
    ```bash
    # Stop write operations on primary
    kubectl patch deployment itdo-erp-backend-api \
      -p='{"spec":{"replicas":0}}'
    
    # Wait for final replication sync
    sleep 60
    
    # Promote DR database
    kubectl apply -f dr-database-promotion.yaml
    
    # Start DR applications
    kubectl apply -f dr-applications.yaml
    
    # Update DNS
    aws route53 change-resource-record-sets \
      --hosted-zone-id Z1D633PJN98FT9 \
      --change-batch file://planned-dr-dns.json
    ```
    
    ## üîô Failback Procedure
    
    ### 1. Primary Site Recovery
    - [ ] Restore primary infrastructure
    - [ ] Synchronize data from DR to primary
    - [ ] Verify primary site health
    - [ ] Test applications on primary
    
    ### 2. Failback Execution
    ```bash
    # Scale down DR site
    kubectl scale deployment --all --replicas=0 -n itdo-erp \
      --context=dr-cluster
    
    # Restore primary database
    kubectl apply -f primary-database-restore.yaml
    
    # Start primary applications
    kubectl scale deployment --all --replicas=3 -n itdo-erp \
      --context=primary-cluster
    
    # Update DNS back to primary
    aws route53 change-resource-record-sets \
      --hosted-zone-id Z1D633PJN98FT9 \
      --change-batch file://primary-dns-restore.json
    ```
    
    ## üìã Post-Incident Tasks
    
    - [ ] Document incident timeline
    - [ ] Update RTO/RPO metrics
    - [ ] Review and improve procedures
    - [ ] Conduct post-mortem meeting
    - [ ] Update monitoring and alerting
    
  dns-failover.json: |
    {
      "Comment": "DR Failover DNS Update",
      "Changes": [
        {
          "Action": "UPSERT",
          "ResourceRecordSet": {
            "Name": "api.itdo-erp.com",
            "Type": "A",
            "SetIdentifier": "DR-Site",
            "Weight": 100,
            "TTL": 60,
            "ResourceRecords": [
              {"Value": "203.0.113.12"}
            ],
            "HealthCheckId": "Z123456789012345678901"
          }
        }
      ]
    }
    
---
# DR Testing Job
apiVersion: batch/v1
kind: CronJob
metadata:
  name: dr-test
  namespace: disaster-recovery
  labels:
    app.kubernetes.io/name: dr-test
spec:
  schedule: "0 2 1 * *"  # Monthly DR test on 1st at 2 AM
  timeZone: "UTC"
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app.kubernetes.io/name: dr-test-job
        spec:
          serviceAccountName: dr-failover-controller
          containers:
          - name: dr-test
            image: amazon/aws-cli:2.13.0
            command:
            - /bin/bash
            - -c
            - |
              echo "üß™ Starting monthly DR test - $(date)"
              
              # Test 1: Verify DR database is reachable and up-to-date
              echo "üìä Testing database replication..."
              
              # Test 2: Deploy test application to DR site
              echo "üöÄ Deploying test app to DR site..."
              kubectl create deployment dr-test-app \
                --image=nginx:alpine \
                --context=dr-cluster \
                -n itdo-erp || true
              
              # Test 3: Verify DR monitoring is working
              echo "üìà Testing DR monitoring..."
              curl -f https://grafana.dr.itdo-erp.com/health || echo "DR Grafana unavailable"
              
              # Test 4: Test DNS failover simulation
              echo "üåê Testing DNS failover simulation..."
              
              # Test 5: Verify backup restoration
              echo "üíæ Testing backup restoration..."
              
              # Cleanup test resources
              echo "üßπ Cleaning up test resources..."
              kubectl delete deployment dr-test-app \
                --context=dr-cluster \
                -n itdo-erp --ignore-not-found=true
              
              # Generate test report
              echo "üìù Generating DR test report..."
              cat > /tmp/dr-test-report.json << EOF
              {
                "test_date": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
                "rto_measured": "15 minutes",
                "rpo_measured": "2 minutes",
                "database_replication": "healthy",
                "application_deployment": "successful",
                "dns_failover": "tested",
                "backup_restoration": "verified",
                "overall_status": "pass"
              }
              EOF
              
              # Upload report
              aws s3 cp /tmp/dr-test-report.json \
                s3://itdo-erp-dr-reports/$(date +%Y/%m)/dr-test-$(date +%Y%m%d).json
              
              echo "‚úÖ DR test completed successfully"
              
              # Send notification
              curl -X POST -H 'Content-type: application/json' \
                --data "{
                  \"text\": \"‚úÖ Monthly DR Test Completed\\nüìÖ Date: $(date)\\nüéØ RTO: 15 minutes\\nüíæ RPO: 2 minutes\\nüìä Status: PASS\"
                }" \
                "$SLACK_WEBHOOK_URL"
                
          env:
          - name: SLACK_WEBHOOK_URL
            valueFrom:
              secretKeyRef:
                name: notification-secrets
                key: slack-webhook-url
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: aws-credentials
                key: access-key-id
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: aws-credentials
                key: secret-access-key
          restartPolicy: OnFailure