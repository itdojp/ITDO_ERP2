#!/bin/bash
# CC03 v63.0 - é«˜åº¦ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ 
# Day 2: è‡ªå‹•åŒ–ãƒ»æš—å·åŒ–ãƒ»ã‚¯ãƒ©ã‚¦ãƒ‰å¯¾å¿œãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

set -euo pipefail

# è¨­å®š
BACKUP_DIR="${BACKUP_DIR:-/backup}"
RETENTION_DAYS="${BACKUP_RETENTION_DAYS:-30}"
ENCRYPTION_KEY="${BACKUP_ENCRYPTION_KEY:-default_key_change_me}"
S3_BUCKET="${S3_BUCKET:-itdo-erp-v63-backups}"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# ãƒ­ã‚°é–¢æ•°
log_info() { echo -e "\033[36m[$(date +'%Y-%m-%d %H:%M:%S')] INFO:\033[0m $1"; }
log_success() { echo -e "\033[32m[$(date +'%Y-%m-%d %H:%M:%S')] SUCCESS:\033[0m $1"; }
log_warn() { echo -e "\033[33m[$(date +'%Y-%m-%d %H:%M:%S')] WARN:\033[0m $1"; }
log_error() { echo -e "\033[31m[$(date +'%Y-%m-%d %H:%M:%S')] ERROR:\033[0m $1"; }

# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
trap 'log_error "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ (Line: $LINENO)"' ERR

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
create_backup_structure() {
    local backup_path="${BACKUP_DIR}/${TIMESTAMP}"
    mkdir -p "${backup_path}"/{database,redis,config,logs,volumes}
    echo "${backup_path}"
}

# PostgreSQLãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
backup_postgresql() {
    local backup_path=$1
    log_info "PostgreSQLãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é–‹å§‹..."
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— (ã‚«ã‚¹ã‚¿ãƒ å½¢å¼)
    pg_dump -h postgres -U "${POSTGRES_USER}" -d "${POSTGRES_DB}" \
        --format=custom \
        --compress=9 \
        --verbose \
        --file="${backup_path}/database/postgres_${TIMESTAMP}.dump"
    
    # ã‚¹ã‚­ãƒ¼ãƒžã®ã¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    pg_dump -h postgres -U "${POSTGRES_USER}" -d "${POSTGRES_DB}" \
        --schema-only \
        --file="${backup_path}/database/postgres_schema_${TIMESTAMP}.sql"
    
    # ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    pg_dumpall -h postgres -U "${POSTGRES_USER}" \
        --globals-only \
        --file="${backup_path}/database/postgres_globals_${TIMESTAMP}.sql"
    
    log_success "PostgreSQLãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†"
}

# Redisãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
backup_redis() {
    local backup_path=$1
    log_info "Redisãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é–‹å§‹..."
    
    # RDB snapshotä½œæˆ
    redis-cli -h redis -a "${REDIS_PASSWORD}" --rdb "${backup_path}/redis/redis_${TIMESTAMP}.rdb"
    
    # Redisè¨­å®šãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    redis-cli -h redis -a "${REDIS_PASSWORD}" CONFIG GET '*' > "${backup_path}/redis/redis_config_${TIMESTAMP}.txt"
    
    # Redisæƒ…å ±ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    redis-cli -h redis -a "${REDIS_PASSWORD}" INFO ALL > "${backup_path}/redis/redis_info_${TIMESTAMP}.txt"
    
    log_success "Redisãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†"
}

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
backup_configs() {
    local backup_path=$1
    log_info "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é–‹å§‹..."
    
    # Docker Composeè¨­å®š
    cp "${SCRIPT_DIR}/../docker-compose.v63-production.yml" "${backup_path}/config/"
    cp "${SCRIPT_DIR}/../.env.v63-production" "${backup_path}/config/"
    
    # Nginxè¨­å®š
    cp -r "${SCRIPT_DIR}/../config/nginx-v63.conf" "${backup_path}/config/"
    
    # SSLè¨¼æ˜Žæ›¸
    if [[ -d "${SCRIPT_DIR}/../config/ssl" ]]; then
        cp -r "${SCRIPT_DIR}/../config/ssl" "${backup_path}/config/"
    fi
    
    # Prometheusè¨­å®š
    cp "${SCRIPT_DIR}/../config/prometheus-v63.yml" "${backup_path}/config/"
    cp "${SCRIPT_DIR}/../config/alert-rules-v63.yml" "${backup_path}/config/"
    
    # Grafanaè¨­å®š
    if [[ -d "${SCRIPT_DIR}/../config/grafana-v63" ]]; then
        cp -r "${SCRIPT_DIR}/../config/grafana-v63" "${backup_path}/config/"
    fi
    
    log_success "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†"
}

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
backup_logs() {
    local backup_path=$1
    log_info "ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é–‹å§‹..."
    
    # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°
    if docker volume ls | grep -q app-logs; then
        docker run --rm -v app-logs:/source -v "${backup_path}/logs":/backup alpine \
            tar -czf /backup/app-logs_${TIMESTAMP}.tar.gz -C /source .
    fi
    
    # Nginxãƒ­ã‚°
    if docker volume ls | grep -q nginx-logs; then
        docker run --rm -v nginx-logs:/source -v "${backup_path}/logs":/backup alpine \
            tar -czf /backup/nginx-logs_${TIMESTAMP}.tar.gz -C /source .
    fi
    
    log_success "ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†"
}

# Dockerãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
backup_volumes() {
    local backup_path=$1
    log_info "Dockerãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é–‹å§‹..."
    
    # PostgreSQLãƒ‡ãƒ¼ã‚¿
    if docker volume ls | grep -q postgres-data; then
        docker run --rm -v postgres-data:/source -v "${backup_path}/volumes":/backup alpine \
            tar -czf /backup/postgres-data_${TIMESTAMP}.tar.gz -C /source .
    fi
    
    # Redisãƒ‡ãƒ¼ã‚¿
    if docker volume ls | grep -q redis-data; then
        docker run --rm -v redis-data:/source -v "${backup_path}/volumes":/backup alpine \
            tar -czf /backup/redis-data_${TIMESTAMP}.tar.gz -C /source .
    fi
    
    # Grafanaãƒ‡ãƒ¼ã‚¿
    if docker volume ls | grep -q grafana-data; then
        docker run --rm -v grafana-data:/source -v "${backup_path}/volumes":/backup alpine \
            tar -czf /backup/grafana-data_${TIMESTAMP}.tar.gz -C /source .
    fi
    
    # Prometheusãƒ‡ãƒ¼ã‚¿
    if docker volume ls | grep -q prometheus-data; then
        docker run --rm -v prometheus-data:/source -v "${backup_path}/volumes":/backup alpine \
            tar -czf /backup/prometheus-data_${TIMESTAMP}.tar.gz -C /source .
    fi
    
    log_success "Dockerãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†"
}

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æš—å·åŒ–
encrypt_backup() {
    local backup_path=$1
    log_info "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æš—å·åŒ–é–‹å§‹..."
    
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä½œæˆ
    local archive_name="itdo-erp-v63-backup_${TIMESTAMP}.tar.gz"
    tar -czf "${BACKUP_DIR}/${archive_name}" -C "${BACKUP_DIR}" "$(basename "${backup_path}")"
    
    # æš—å·åŒ–
    openssl enc -aes-256-cbc -salt -in "${BACKUP_DIR}/${archive_name}" \
        -out "${BACKUP_DIR}/${archive_name}.enc" \
        -pass pass:"${ENCRYPTION_KEY}"
    
    # å…ƒã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å‰Šé™¤
    rm "${BACKUP_DIR}/${archive_name}"
    
    # ãƒã‚§ãƒƒã‚¯ã‚µãƒ ä½œæˆ
    sha256sum "${BACKUP_DIR}/${archive_name}.enc" > "${BACKUP_DIR}/${archive_name}.enc.sha256"
    
    log_success "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æš—å·åŒ–å®Œäº†: ${archive_name}.enc"
    echo "${BACKUP_DIR}/${archive_name}.enc"
}

# ã‚¯ãƒ©ã‚¦ãƒ‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (AWS S3)
upload_to_cloud() {
    local encrypted_file=$1
    
    if [[ -z "${AWS_ACCESS_KEY_ID:-}" || -z "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
        log_warn "AWSèªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚¯ãƒ©ã‚¦ãƒ‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
        return 0
    fi
    
    log_info "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹..."
    
    # AWS CLIä½¿ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
    if command -v aws &> /dev/null; then
        aws s3 cp "${encrypted_file}" "s3://${S3_BUCKET}/$(basename "${encrypted_file}")"
        aws s3 cp "${encrypted_file}.sha256" "s3://${S3_BUCKET}/$(basename "${encrypted_file}.sha256")"
        log_success "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†"
    else
        log_warn "AWS CLIãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚curlã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’è©¦è¡Œã—ã¾ã™ã€‚"
        # ã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ï¼ˆå¿…è¦ã«å¿œã˜ã¦å®Ÿè£…ï¼‰
    fi
}

# å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤
cleanup_old_backups() {
    log_info "å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹..."
    
    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    find "${BACKUP_DIR}" -name "*.enc" -type f -mtime +${RETENTION_DAYS} -delete
    find "${BACKUP_DIR}" -name "*.sha256" -type f -mtime +${RETENTION_DAYS} -delete
    find "${BACKUP_DIR}" -maxdepth 1 -type d -mtime +${RETENTION_DAYS} -exec rm -rf {} +
    
    # S3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆAWS CLIä½¿ç”¨ï¼‰
    if command -v aws &> /dev/null && [[ -n "${AWS_ACCESS_KEY_ID:-}" ]]; then
        local cutoff_date=$(date -d "${RETENTION_DAYS} days ago" +%Y-%m-%d)
        aws s3 ls "s3://${S3_BUCKET}/" | while read -r line; do
            local file_date=$(echo "${line}" | awk '{print $1}')
            local file_name=$(echo "${line}" | awk '{print $4}')
            
            if [[ "${file_date}" < "${cutoff_date}" ]]; then
                aws s3 rm "s3://${S3_BUCKET}/${file_name}"
                log_info "å‰Šé™¤ã•ã‚ŒãŸS3ãƒ•ã‚¡ã‚¤ãƒ«: ${file_name}"
            fi
        done
    fi
    
    log_success "å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†"
}

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œè¨¼
verify_backup() {
    local encrypted_file=$1
    log_info "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œè¨¼é–‹å§‹..."
    
    # ãƒã‚§ãƒƒã‚¯ã‚µãƒ æ¤œè¨¼
    if sha256sum -c "${encrypted_file}.sha256"; then
        log_success "ãƒã‚§ãƒƒã‚¯ã‚µãƒ æ¤œè¨¼æˆåŠŸ"
    else
        log_error "ãƒã‚§ãƒƒã‚¯ã‚µãƒ æ¤œè¨¼å¤±æ•—"
        return 1
    fi
    
    # æš—å·åŒ–ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
    local test_decrypt="/tmp/backup_test_decrypt"
    if openssl enc -d -aes-256-cbc -in "${encrypted_file}" \
        -out "${test_decrypt}" \
        -pass pass:"${ENCRYPTION_KEY}" 2>/dev/null; then
        
        if file "${test_decrypt}" | grep -q "gzip compressed"; then
            log_success "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼æˆåŠŸ"
            rm -f "${test_decrypt}"
        else
            log_error "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼å¤±æ•—"
            rm -f "${test_decrypt}"
            return 1
        fi
    else
        log_error "å¾©å·åŒ–ãƒ†ã‚¹ãƒˆå¤±æ•—"
        return 1
    fi
}

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
record_backup_metrics() {
    local backup_path=$1
    local encrypted_file=$2
    
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚µã‚¤ã‚ºè¨ˆç®—
    local backup_size=$(du -sb "${backup_path}" | cut -f1)
    local encrypted_size=$(stat -c%s "${encrypted_file}")
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ï¼ˆPrometheuså½¢å¼ï¼‰
    cat >> "${BACKUP_DIR}/backup_metrics.prom" << EOF
# HELP backup_size_bytes Size of backup in bytes
# TYPE backup_size_bytes gauge
backup_size_bytes{type="raw"} ${backup_size}
backup_size_bytes{type="encrypted"} ${encrypted_size}

# HELP backup_timestamp_seconds Timestamp of backup completion
# TYPE backup_timestamp_seconds gauge
backup_timestamp_seconds $(date +%s)

# HELP backup_success Boolean indicating backup success
# TYPE backup_success gauge
backup_success 1
EOF

    log_info "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²å®Œäº†"
}

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
main() {
    log_info "ðŸ”„ CC03 v63.0 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é–‹å§‹ (${TIMESTAMP})"
    
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ§‹é€ ä½œæˆ
    local backup_path=$(create_backup_structure)
    
    # å„ç¨®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
    backup_postgresql "${backup_path}"
    backup_redis "${backup_path}"
    backup_configs "${backup_path}"
    backup_logs "${backup_path}"
    backup_volumes "${backup_path}"
    
    # æš—å·åŒ–
    local encrypted_file=$(encrypt_backup "${backup_path}")
    
    # æ¤œè¨¼
    verify_backup "${encrypted_file}"
    
    # ã‚¯ãƒ©ã‚¦ãƒ‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    upload_to_cloud "${encrypted_file}"
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
    record_backup_metrics "${backup_path}" "${encrypted_file}"
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    cleanup_old_backups
    
    # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤
    rm -rf "${backup_path}"
    
    log_success "ðŸŽ‰ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†!"
    log_info "æš—å·åŒ–ãƒ•ã‚¡ã‚¤ãƒ«: $(basename "${encrypted_file}")"
    log_info "ã‚µã‚¤ã‚º: $(du -h "${encrypted_file}" | cut -f1)"
}

# cronå®Ÿè¡Œå¯¾å¿œ
if [[ "${1:-}" == "--cron" ]]; then
    exec > >(logger -t backup-v63) 2>&1
fi

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi